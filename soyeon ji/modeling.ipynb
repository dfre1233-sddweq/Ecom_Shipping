{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# E-Commerce Shipping - Baseline Model Training\n",
                "\n",
                "이 노트북은 전처리된 데이터를 바탕으로 베이스라인 모델을 학습하고 평가하는 과정을 담고 있습니다.\n",
                "\n",
                "**주요 내용:**\n",
                "1. 데이터 로드 및 전처리 (Preprocessing Pipeline)\n",
                "2. 데이터 분할 (Train/Test Split)\n",
                "3. 베이스라인 모델 학습 (Logistic Regression, Decision Tree, Random Forest)\n",
                "4. 모델 평가 (Accuracy, F1-score, Confusion Matrix)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "\n",
                "# 한글 폰트 설정\n",
                "if os.name == 'nt':\n",
                "    plt.rc('font', family='Malgun Gothic')\n",
                "elif os.name == 'posix':\n",
                "    plt.rc('font', family='AppleGothic')\n",
                "else:\n",
                "    print(\"Unknown OS\")\n",
                "\n",
                "plt.rc('axes', unicode_minus=False)\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. 데이터 로드 및 전처리 (Data Loading & Preprocessing)\n",
                "앞서 `preprocessing.ipynb`에서 정의한 전처리 파이프라인을 적용합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 데이터 로드\n",
                "df = pd.read_csv('data/Train.csv')\n",
                "\n",
                "# ID 삭제\n",
                "if 'ID' in df.columns:\n",
                "    df = df.drop('ID', axis=1)\n",
                "\n",
                "# 타겟 변수 이름 변경\n",
                "if 'Reached.on.Time_Y.N' in df.columns:\n",
                "    df = df.rename(columns={'Reached.on.Time_Y.N': 'target'})\n",
                "\n",
                "# --- Feature Engineering ---\n",
                "# 1. 할인율\n",
                "df['Discount_Rate'] = (df['Discount_offered'] / df['Cost_of_the_Product']) * 100\n",
                "\n",
                "# 2. 고할인 여부\n",
                "df['Is_High_Discount'] = (df['Discount_offered'] > 10).astype(int)\n",
                "\n",
                "# 3. 무게당 비용\n",
                "df['Cost_per_Weight'] = df['Cost_of_the_Product'] / df['Weight_in_gms']\n",
                "\n",
                "# 4. 무게 구간화\n",
                "bins = [0, 2000, 4000, 6000, float('inf')]\n",
                "labels = ['Low', 'Medium', 'High', 'Very High']\n",
                "df['Weight_Range'] = pd.cut(df['Weight_in_gms'], bins=bins, labels=labels)\n",
                "\n",
                "# --- Preprocessing ---\n",
                "# Encoding\n",
                "df['Gender'] = df['Gender'].map({'F': 0, 'M': 1})\n",
                "df['Product_importance'] = df['Product_importance'].map({'low': 0, 'medium': 1, 'high': 2})\n",
                "\n",
                "categorical_cols = ['Warehouse_block', 'Mode_of_Shipment', 'Weight_Range']\n",
                "df = pd.get_dummies(df, columns=categorical_cols, drop_first=False)\n",
                "\n",
                "# Scaling\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "scaler = StandardScaler()\n",
                "numeric_cols = ['Cost_of_the_Product', 'Weight_in_gms', 'Discount_offered', \n",
                "                'Prior_purchases', 'Customer_care_calls', 'Customer_rating',\n",
                "                'Discount_Rate', 'Cost_per_Weight']\n",
                "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
                "\n",
                "print(\"전처리 완료. 데이터 크기:\", df.shape)\n",
                "display(df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. 데이터 분할 (Train/Test Split)\n",
                "학습 데이터와 검증 데이터로 분리합니다 (80:20)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "X = df.drop('target', axis=1)\n",
                "y = df['target']\n",
                "\n",
                "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "print(f\"Train set: {X_train.shape}, Valid set: {X_valid.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. 베이스라인 모델 학습 (Baseline Models)\n",
                "가장 기본적인 모델인 로지스틱 회귀, 의사결정나무, 랜덤포레스트를 학습시킵니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
                "\n",
                "# 모델 정의\n",
                "models = {\n",
                "    'Logistic Regression': LogisticRegression(random_state=42),\n",
                "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
                "    'Random Forest': RandomForestClassifier(random_state=42)\n",
                "}\n",
                "\n",
                "# 학습 및 평가 함수\n",
                "def train_evaluate(models, X_train, y_train, X_valid, y_valid):\n",
                "    results = {}\n",
                "    for name, model in models.items():\n",
                "        model.fit(X_train, y_train)\n",
                "        y_pred = model.predict(X_valid)\n",
                "        \n",
                "        acc = accuracy_score(y_valid, y_pred)\n",
                "        f1 = f1_score(y_valid, y_pred)\n",
                "        \n",
                "        results[name] = {'Accuracy': acc, 'F1 Score': f1, 'Model': model}\n",
                "        \n",
                "        print(f\"--- {name} ---\")\n",
                "        print(f\"Accuracy: {acc:.4f}\")\n",
                "        print(f\"F1 Score: {f1:.4f}\")\n",
                "        print(classification_report(y_valid, y_pred))\n",
                "        print(\"-\" * 30)\n",
                "        \n",
                "    return results\n",
                "\n",
                "results = train_evaluate(models, X_train, y_train, X_valid, y_valid)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. 모델 평가 시각화 (Evaluation Visualization)\n",
                "각 모델의 혼동 행렬(Confusion Matrix)과 Feature Importance를 시각화합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "for i, (name, result) in enumerate(results.items()):\n",
                "    model = result['Model']\n",
                "    y_pred = model.predict(X_valid)\n",
                "    cm = confusion_matrix(y_valid, y_pred)\n",
                "    \n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i])\n",
                "    axes[i].set_title(f'{name} Confusion Matrix')\n",
                "    axes[i].set_xlabel('Predicted')\n",
                "    axes[i].set_ylabel('Actual')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Random Forest Feature Importance\n",
                "rf_model = results['Random Forest']['Model']\n",
                "importances = rf_model.feature_importances_\n",
                "indices = np.argsort(importances)[::-1]\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "plt.title(\"Random Forest Feature Importance\")\n",
                "plt.bar(range(X.shape[1]), importances[indices], align=\"center\")\n",
                "plt.xticks(range(X.shape[1]), X.columns[indices], rotation=90)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. 결론 (Conclusion)\n",
                "다음 단계로는 하이퍼파라미터 튜닝이나 앙상블 모델링을 통해 성능을 개선할 수 있습니다."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}